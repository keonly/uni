{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS492(I) Assignment #1: Image Classification using Convolutional Neural Networks (CNNs) \n",
    "---\n",
    "TA : Jinwoo Kim (jinwoo-kim@kaist.ac.kr), Jaehoon Yoo (wogns98@kaist.ac.kr)\n",
    "\n",
    "---\n",
    "\n",
    "## Instructions\n",
    "- In this assignment, we will classify the images in CIFAR10 dataset into 10 categories (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) using Convolutional Neural Networks(CNNs).  \n",
    "\n",
    "- To this end, you need to implement necessary network components (e.g. residual blocks) using nn.Module class and complete whole CNNs with those blocks. Then, you will experiment those network architectures using given train/testing pipeline and report classfication accuracies on the test set.      \n",
    "\n",
    "- In each part, you will be given a starter code for the implementation. Please read the attached illustrations and instructions carefully to implement the codes.  \n",
    "\n",
    "- As you follow the given steps, fill in the section marked ***Px.x*** (e.g. P1.1, P1.2, etc) with the appropriate code. **Note that you can only fill those marked areas, and cannot modify rest of the  skeleton code.**  \n",
    "\n",
    "- In short, you should (1) complete the code, (2) experiment with several configurations of CNNs, and (3) report the final classification accuracies on the CIFAR10 test set.\n",
    "- To start with, you should download this ipynb file into your own google drive.\n",
    "You can save the file into your own google drive by clicking `make a copy(사본만들기)`. Find the copy in your drive, change their name to `assignment1.ipynb`, if their names were changed to e.g. `Copy of assignment1.ipyb` or `assignment1.ipynb의 사본`. \n",
    "\n",
    "## Submission guidelines\n",
    "- Your code and report will be all in Colab. \n",
    "- <font color=\"red\"> You will get the full credit **only if** you complete the code **and** match your results(classifiction accuarcy, number of parameter) with the values we provided on the bottom of the project. Once you trained all the models, we will automatically collect the experimental results.\n",
    " </font>\n",
    "- <font color=\"red\"> Again, do not modify the skeleton codes. Only write your code inside the designated area. </font>\n",
    "- You may download this notebook to run the code on a local machine. However, we should be able to reproduce your results on Colab using your code. Please double-check if your code runs without error and reproduces your results **on Colab**. Submissions failed to run or reproduce the results will get a substantial penalty. \n",
    "\n",
    "## Deliverables\n",
    "- Download the following files from your google drive, and submit them in a zip file named as **[StudentID].zip**. For example, if your student ID is 20201234, the file name should be **20201234.zip**.\n",
    "  - **[StudentID].ipynb**: Your Colab notebook.\n",
    "  - **[StudentID]_[model].pt**: the model checkpoints. You have to submit **4** models in total: `conv_best.pt`, `resPlain_best.pt`, `resBottleneck_best.pt`, and `inception_best.pt`.\n",
    " </font>\n",
    "<font color=\"red\">\n",
    "- If your submission doesn't match the above deliverable, your score will be deducted!</font>\n",
    "- Your assignment should be submitted through **KLMS**. All other submissions (e.g., via email) will not be considered as valid submissions.\n",
    "\n",
    "## Plagiarism\n",
    "**Plagiarism will not be tolerated**, and will be heavily penalized by an F for the course and by reporting to the head professor of student affairs as well as the head of School of Computing.\n",
    "We use the following criteria, as in [School of Computing Honor Code 2021 Autumn](https://docs.google.com/forms/d/e/1FAIpQLScyucDgf7Uyp2FX0rUsU1GcGBBey-i-jCBDt9TECWRb3DeWWg/viewform):\n",
    "- Reusing, or referring to other students/publisher’s solutions, assignments, program source code, and reports\n",
    "- Using solution-sharing online services such as chegg.com for exams and assignments\n",
    "- Allowing another student to refer from one’s own work\n",
    "- Submitting another student’s work as his or her own\n",
    "- Unpermitted collaboration or aid on take-home examinations and class assignments\n",
    "- The use of another person's original work, regardless of lengths, without giving reasonable and appropriate credit to or acknowledging the author or source\n",
    "\n",
    "We will use multiple tools for plagiarism checking. So please don't cheat. \n",
    "\n",
    "## Due date\n",
    "- **23:59:59 October 8th.** \n",
    "- Late submission is allowed until 23:59:59 October 10th.\n",
    "- Late submissions will have 20% penalty.\n",
    "- You have 2 days of grace period throughout this course. The grace period will be used automatically. You don't need to notify it to TAs.\n",
    "\n",
    "\n",
    "## Questions\n",
    "- Please use the Slack ```#assignment1``` QnA channel as the main communication channel.\n",
    "- When you post questions, please make it public so that all students can share the information.\n",
    "- Please use the prefix \"[Assignment 1]\" in the subject for all questions regarding this assignment (e.g., [Assignment 1] Regarding the grading policy).\n",
    "\n",
    "## Changelog\n",
    "None so far.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Prerequisite: change the runtime type to **GPU**.\n",
    "\n",
    "![test](https://docs.google.com/uc?export=download&id=1Jugrjl86L9EY1ePTjH8OVMFq7gmZsoz_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prerequisite: mount your gdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n",
    "# login with your google account and type authorization code to mount on your google drive.\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Prerequisite: setup the `root` directory properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory path where `assignemnt1.ipynb` exists.\n",
    "# For example, if you saved `assignment1.ipynb` in `/gdrive/My Drive/CS492I/assignment1` directory,\n",
    "# then set root = '/gdrive/My Drive/CS492I/assignment1'\n",
    "root = '/gdrive/My Drive/Colab Notebooks/CS492I/assignment1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# 1.Implementing Network Modules\n",
    "\n",
    "In this assignment, you will implement four modularized blocks and one network class as follows:\n",
    "\n",
    "**Block classes**  \n",
    "(Example) Multilayer perceptron Block (MLPBlock) **To provide a starting point, the solutions for this section are given below.**  \n",
    "(1) Convolutional block (ConvBlock)   \n",
    "(2) Plain residual block (ResBlockPlain)  \n",
    "(3) Residual block with bottleneck (ResBlockBottleneck)  \n",
    "(4) Inception Block (InceptionBlock)\n",
    "\n",
    "**Network class**  \n",
    "(1) MyNetwork \n",
    "\n",
    "In each cell, there is a starter code, a schematic illustration, and instructions that will guide you to implement each module correctly. Specifically, the schematic illustrations are to show you the computational graphs of modules, which give you high-level views on how the modules should be constructed and work. (E.g. which nn.Module to use, or input/output shape of each layer written in italics). Therefore, please read the illustrations and instructions carefully to complete the codes.\n",
    "<!-- \n",
    "Below is an example.\n",
    "\n",
    "### Example: ConvLayer Module [(Illustration)](https://docs.google.com/drawings/d/1_aPhPSPgh5-5FEfI_jnfp8r6-wNjY_QYXBT3zzjkHk0/edit?usp=sharing) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Example) Implement MLP Block [(Illustration)](https://docs.google.com/drawings/d/1gTPLeK0H5ooMcn7CNPysqwr9_07fTqkHE4-T3ZqyhPo/edit?usp=sharing)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a basic multi-layer perceptron module components.\n",
    "        Illustration: https://docs.google.com/drawings/d/1gTPLeK0H5ooMcn7CNPysqwr9_07fTqkHE4-T3ZqyhPo/edit?usp=sharing\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n",
    "            2. Initialized network components will be referred in `forward` method \n",
    "               for constructing the dynamic computational graph.\n",
    "        \n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in input.\n",
    "            2. out_channels (int): Number of channels to be produced.\n",
    "        \"\"\"\n",
    "        #######################################\n",
    "        ## This section is an example.       ##        \n",
    "        self.fc1 = nn.Linear(in_channels, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, out_channels)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        self.act = nn.ReLU()\n",
    "        #######################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed-forward data 'x' through the module.\n",
    "                \n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link \n",
    "               using the initialized components in __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): A tensor of shape (B, in_channels)\n",
    "            .\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, out_channels). \n",
    "        \"\"\"\n",
    "        #######################################\n",
    "        ## This section is an example.       ##\n",
    "        output = self.act(self.bn1(self.fc1(x)))\n",
    "        output = self.act(self.bn2(self.fc2(output)))\n",
    "        output = self.act(self.bn3(self.fc3(output)))\n",
    "        #######################################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Implement Convolutional Block[(Illustration)](https://docs.google.com/drawings/d/1MRYBywpuazlldwC11UTa-kuWMWEDsFewDnirKiFX5us/edit?usp=sharing) (10pt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, \n",
    "                 padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize a basic convolutional layer module components.\n",
    "        Illustration: https://docs.google.com/drawings/d/1MRYBywpuazlldwC11UTa-kuWMWEDsFewDnirKiFX5us/edit?usp=sharing\n",
    "\n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in the input. \n",
    "            2. out_channels (int): Number of channels produced.\n",
    "            3. kernel_size (int) : Size of the kernel used in conv layer (Default:3)\n",
    "            4. stride (int) : Stride of the convolution (Default:1)\n",
    "            5. padding (int) : Zero-padding added to both sides of the input (Default:1)\n",
    "        \"\"\"\n",
    "        #################################\n",
    "        ## P1.1. Write your code here  ##\n",
    "\n",
    "        self.layer = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "                                   nn.BatchNorm2d(out_channels), \n",
    "                                   nn.ReLU())\n",
    "\n",
    "        #################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed-forward the data 'x' through the module.\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link \n",
    "               using the initialized components in __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): A tensor of shape (B, in_channels, H, W).\n",
    "            \n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, out_channels, H, W). \n",
    "        \"\"\"\n",
    "        #################################\n",
    "        ## P1.2. Write your code here  ##       \n",
    "\n",
    "        output = self.layer(x)\n",
    "\n",
    "        #################################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Implement ResBlockPlain [(Illustration)](https://docs.google.com/drawings/d/19FS5w7anbTAF6UrMPdM4fs8nk9x3Lm5KRIODawC4duQ/edit?usp=sharing) (10pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockPlain(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResBlockPlain, self).__init__()\n",
    "        \"\"\"Initialize a residual block module components.\n",
    "\n",
    "        Illustration: https://docs.google.com/drawings/d/19FS5w7anbTAF6UrMPdM4fs8nk9x3Lm5KRIODawC4duQ/edit?usp=sharing\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n",
    "            2. Initialized network components will be referred in `forward` method \n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in the input.\n",
    "        \"\"\"\n",
    "        #################################\n",
    "        ## P2.1. Write your code here ##\n",
    "\n",
    "        self.layer = nn.Sequential(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                   nn.BatchNorm2d(in_channels),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                   nn.BatchNorm2d(in_channels) )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        #################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the network.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link \n",
    "               using the initialized components in __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): An tensor of shape (B, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, in_channels, H, W). \n",
    "        \"\"\"\n",
    "        ################################\n",
    "        ## P2.2. Write your code here ## \n",
    "\n",
    "        output = self.layer(x)\n",
    "        output += self.shortcut(x)\n",
    "        output = self.act(output)\n",
    "\n",
    "        ################################\n",
    "        return output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Implement ResBlockBottleneck [(Illustration)](https://docs.google.com/drawings/d/1n2E0TwiWhf1IGdD16-MeQjzUcys_V7ETTzn33j_bEy0/edit?usp=sharing) (10pt)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlockBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(ResBlockBottleneck, self).__init__()\n",
    "        \"\"\"Initialize a residual block module components.\n",
    "\n",
    "        Illustration: https://docs.google.com/drawings/d/1n2E0TwiWhf1IGdD16-MeQjzUcys_V7ETTzn33j_bEy0/edit?usp=sharing\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n",
    "            2. Initialized network components will be referred in `forward` method \n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in the input. \n",
    "            2. hidden_channels (int): Number of hidden channels produced by the first ConvLayer module.\n",
    "        \"\"\"\n",
    "        #################################\n",
    "        ## P3.1. Write your code here  ##\n",
    "\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                                   nn.BatchNorm2d(hidden_channels),\n",
    "                                   nn.ReLU() )\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(hidden_channels, hidden_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                    nn.BatchNorm2d(hidden_channels),\n",
    "                                    nn.ReLU() )\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(hidden_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                                    nn.BatchNorm2d(in_channels) )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        #################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the network.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link \n",
    "               using the initialized components in __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): An tensor of shape (B, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, in_channels, H, W). \n",
    "        \"\"\"\n",
    "        ################################\n",
    "        ## P3.2. Write your code here ##\n",
    "\n",
    "        output = self.layer1(x)\n",
    "        output = self.layer2(output)\n",
    "        output = self.layer3(output)\n",
    "        output += self.shortcut(x)\n",
    "        output = self.act(output)\n",
    "\n",
    "        ################################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Implement InceptionBlock[(Illustration)](https://docs.google.com/drawings/d/1I020R1YqVAr8LWKHgm7N5J5fzFpHvx1fqXuAs6z8qyE/edit?usp=sharing)  (20pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(InceptionBlock, self).__init__()\n",
    "        \"\"\"Initialize a basic InpcetionBlock module components.\n",
    "\n",
    "        Illustration: https://docs.google.com/drawings/d/1I020R1YqVAr8LWKHgm7N5J5fzFpHvx1fqXuAs6z8qyE/edit?usp=sharing\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n",
    "            2. Initialized network components will be referred in `forward` method \n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. in_channels (int): Number of channels in the input. \n",
    "            2. out_channels (int): Number of channels in the final output.\n",
    "        \"\"\"\n",
    "        assert out_channels%8==0, 'out channel should be mutiplier of 8'\n",
    "\n",
    "        ################################\n",
    "        ## P4.1. Write your code here ##\n",
    "\n",
    "        self.branch1x1 = nn.Sequential(nn.Conv2d(in_channels, out_channels//4, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                                       nn.BatchNorm2d(out_channels//4),\n",
    "                                       nn.ReLU() )\n",
    "        self.branch3x3 = nn.Sequential(nn.Conv2d(in_channels, out_channels//2, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                                       nn.BatchNorm2d(out_channels//2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(out_channels//2, out_channels//2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(out_channels//2),\n",
    "                                       nn.ReLU() )\n",
    "        self.branch5x5 = nn.Sequential(nn.Conv2d(in_channels, out_channels//8, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                                       nn.BatchNorm2d(out_channels//8),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Conv2d(out_channels//8, out_channels//8, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "                                       nn.BatchNorm2d(out_channels//8),\n",
    "                                       nn.ReLU() )\n",
    "        self.maxpool = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "                                     nn.Conv2d(in_channels, out_channels//8, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                                     nn.BatchNorm2d(out_channels//8),\n",
    "                                     nn.ReLU() )\n",
    "\n",
    "        ################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the module.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link \n",
    "               using the initialized components in the __init__ method.\n",
    "\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): A tensor of shape (B, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, out_channels, H, W). \n",
    "\n",
    "        \"\"\"\n",
    "        ################################\n",
    "        ## P4.2. Write your code here ##\n",
    " \n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "        branch5x5 = self.branch5x5(x)\n",
    "        maxpool = self.maxpool(x)\n",
    "        output = torch.cat([branch1x1, branch3x3, branch5x5, maxpool], dim=1)\n",
    " \n",
    "        ################################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Example) MyNetworkExample\n",
    "\n",
    "The class `MyNetworkExample` is a sample network using `MLPBlock` implemented above. **You don't have to implement anything in this code section.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetworkExample(nn.Module):\n",
    "    def __init__(self, nf, block_type='mlp'):\n",
    "        super(MyNetworkExample, self).__init__()\n",
    "        \"\"\"Initialize an entire network module components.\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components. \n",
    "            2. Initialized network components will be referred in `forward` method \n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:        \n",
    "            1. nf (int): Number of input channels for the first nn.Linear Module. An abbreviation for num_filter.\n",
    "            2. block_type (str, optional): Type of blocks to use. ('mlp'. default: 'mlp')\n",
    "        \"\"\"\n",
    "        #######################################\n",
    "        ## This section is an example.       ##\n",
    "        if block_type == 'mlp':\n",
    "            block = MLPBlock\n",
    "            # Since shape of input image is 3 x 32 x 32, the size of flattened input is 3*32*32. \n",
    "            self.mlp = block(3*32*32, nf)\n",
    "            self.fc = nn.Linear(nf, 10)\n",
    "        else:\n",
    "            raise Exception(f\"Wrong type of block: {block_type}.Expected : mlp\")\n",
    "        #######################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the network.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link \n",
    "               using the initialized network components in __init__ method.\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): An image tensor of shape (B, 3, 32, 32).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, 10). \n",
    "        \"\"\"\n",
    "        #######################################\n",
    "        ## This section is an example.       ##\n",
    "        output = self.mlp(x.view(x.size()[0], -1))\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "        #######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) MyNetwork[(Illustration)](https://docs.google.com/drawings/d/1L8PYO8A1EL4BN4bzTWH4ygr-WiS7NDeFz7P1PkhBZwE/edit?usp=sharing) (10pt)\n",
    "\n",
    "There are two functions to implement in this section. **Read the comments and illustration carefully before you type anything.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, nf, block_type='conv', num_blocks=[1, 1, 1]):\n",
    "        super(MyNetwork, self).__init__()\n",
    "        \"\"\"Initialize an entire network module components.\n",
    "\n",
    "        Illustration: https://docs.google.com/drawings/d/1L8PYO8A1EL4BN4bzTWH4ygr-WiS7NDeFz7P1PkhBZwE/edit?usp=sharing\n",
    "\n",
    "        Instructions:\n",
    "            1. Implement an algorithm that initializes necessary components as illustrated in the above link. \n",
    "            2. Initialized network components will be referred in `forward` method \n",
    "               for constructing the dynamic computational graph.\n",
    "\n",
    "        Args:\n",
    "            1. nf (int): Number of output channels for the first nn.Conv2d Module. An abbreviation for num_filter.\n",
    "            2. block_type (str, optional): Type of blocks to use. ('conv' | 'resPlain' | 'resBottleneck' | 'inception'. default: 'conv')\n",
    "            3. num_blocks (list or tuple, optional): A list or tuple of length 3. \n",
    "               Each item at i-th index indicates the number of blocks at i-th Layer.  \n",
    "               (default: [1, 1, 1])\n",
    "        \"\"\"\n",
    "        \n",
    "        self.block_type = block_type\n",
    "\n",
    "        # Define blocks according to block_type\n",
    "        if self.block_type == 'conv':\n",
    "            block = ConvBlock\n",
    "            block_args = lambda x: (x, x, 3, 1, 1)\n",
    "        elif self.block_type == 'resPlain':\n",
    "            block = ResBlockPlain\n",
    "            block_args = lambda x: (x,)\n",
    "        elif self.block_type == 'resBottleneck':\n",
    "            block = ResBlockBottleneck\n",
    "            block_args = lambda x: (x, x//2)\n",
    "        elif self.block_type == 'inception':\n",
    "            block = InceptionBlock\n",
    "            block_args = lambda x: (x, x)\n",
    "        else:\n",
    "            raise Exception(f\"Wrong type of block: {block_type}\")\n",
    "\n",
    "        # Define block layer by stacking multiple blocks. \n",
    "        # You don't need to modify it. Just use these block layers in forward function.  \n",
    "        self.block1 = nn.Sequential(*[block(*block_args(nf)) for _ in range(num_blocks[0])])\n",
    "        self.block2 = nn.Sequential(*[block(*block_args(nf*2)) for _ in range(num_blocks[1])])\n",
    "        self.block3 = nn.Sequential(*[block(*block_args(nf*4)) for _ in range(num_blocks[2])])\n",
    "\n",
    "        ################################\n",
    "        ## P5.1. Write your code here ##\n",
    "\n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3, nf*1, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(nf*1),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool2d(kernel_size=2, stride=2) )\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(nf*1, nf*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(nf*2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool2d(kernel_size=2, stride=2) )       \n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(nf*2, nf*4, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       nn.BatchNorm2d(nf*4),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.MaxPool2d(kernel_size=2, stride=2) )\n",
    "        self.act = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),\n",
    "                                 nn.Flatten(),\n",
    "                                 nn.Linear(nf*4,10) )\n",
    "\n",
    "        ################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Feed-forward the data `x` through the network.\n",
    "\n",
    "        Instructions:\n",
    "            1. Construct the feed-forward computational graph as illustrated in the link \n",
    "               using the initialized network components in __init__ method.\n",
    "        Args:\n",
    "            1. x (torch.FloatTensor): An image tensor of shape (B, 3, 32, 32).\n",
    "\n",
    "        Returns:\n",
    "            1. output (torch.FloatTensor): An output tensor of shape (B, 10). \n",
    "        \"\"\"\n",
    "\n",
    "        #######################################################################\n",
    "        ## P5.2. Write your code here                                        ##\n",
    "        ## Hint : use self.block1, self.block2, self.block3 for block layers ##\n",
    " \n",
    "        output = self.layer1(x)\n",
    "        output = self.block1(output)\n",
    "        output = self.layer2(output)\n",
    "        output = self.block2(output)\n",
    "        output = self.layer3(output)\n",
    "        output = self.block3(output)\n",
    "        output = self.act(output)\n",
    " \n",
    "        #######################################################################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2.Experiment with Train/Test Pipeline\n",
    "\n",
    "This section contains the entire train and test loop of the pipeline, specifically the followings:\n",
    "1. feed inputs into the network, get outputs, and then compute classification loss. \n",
    "2. backward the computed loss and update network weights (only in the training loop).\n",
    "3. save tensorboard logs frequently.\n",
    "4. save checkpoint weights frequently.\n",
    "\n",
    "**There are no modifications necessary in this section.** Run the code and enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments and Environment Settings\n",
    "\n",
    "This section contains code that\n",
    "- defines miscellaneous arguments for our pipeline.\n",
    "- runs Tensorboard to visualize accuracy and loss curves.\n",
    "\n",
    "Optionally, you may change `args.ckpt_iter` and `args.`log_iter` as you wish to save space in your Google Drive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations & Hyper-parameters\n",
    "\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "# set manual seeds \n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "args = edict()\n",
    "\n",
    "# basic options \n",
    "args.name = 'main'                   # experiment name.\n",
    "args.ckpt_dir = 'ckpts'              # checkpoint directory name.\n",
    "args.ckpt_iter = 1000                # how frequently checkpoints are saved.\n",
    "args.ckpt_reload = 'best'            # which checkpoint to re-load.\n",
    "args.gpu = True                      # whether or not to use gpu. \n",
    "\n",
    "# network options\n",
    "args.num_filters = 16                # number of output channels in the first nn.Conv2d module in MyNetwork.\n",
    "args.block_type = 'mlp'              # type of block. ('mlp' | 'conv' | 'resPlain' | 'resBottleneck' | 'inception').\n",
    "args.num_blocks = [5, 5, 5]          # number of blocks in each Layer.\n",
    "\n",
    "# data options\n",
    "args.dataroot = 'dataset/cifar10'    # where CIFAR10 images exist.\n",
    "args.batch_size = 128                # number of mini-batch size.\n",
    "\n",
    "# training options\n",
    "args.lr = 0.1                        # learning rate.\n",
    "args.epoch = 100                     # training epoch.\n",
    "\n",
    "# tensorboard options\n",
    "args.tensorboard = True              # whether or not to use tensorboard logging.\n",
    "args.log_dir = 'logs'                # to which tensorboard logs will be saved.\n",
    "args.log_iter = 100                  # how frequently logs are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "global_step = 0\n",
    "best_accuracy = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train/test data loaders  \n",
    "# Use data augmentation in training set to mitigate overfitting. \n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),                                \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([                       \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ])\n",
    "\n",
    "train_dataset = CIFAR10(args.dataroot, download=True, train=True, transform=train_transform)\n",
    "test_dataset = CIFAR10(args.dataroot, download=True, train=False, transform=test_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking the states with Tensorboard\n",
    "\n",
    "In following training stage, losses and accuracies will be logged on the tensorboard. It provides an useful data for analyzing training process. \n",
    "Use tensorboard wisely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup tensorboard.\n",
    "if args.tensorboard:\n",
    "    from torch.utils.tensorboard import SummaryWriter \n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"/gdrive/My Drive/{str(result_dir).replace('/gdrive/My Drive/', '')}\"\n",
    "else:\n",
    "    writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Train-and-Test pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, optimizer, scheduler, block_type, writer):\n",
    "    global_step = 0\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        # Here starts the train loop.\n",
    "        net.train()\n",
    "        for batch_idx, (x, y) in enumerate(train_dataloader):\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            #  Send `x` and `y` to either cpu or gpu using `device` variable. \n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            # Feed `x` into the network, get an output, and keep it in a variable called `logit`. \n",
    "            logit = net(x)\n",
    "\n",
    "            # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.\n",
    "            accuracy = (logit.argmax(1) == y).float().mean()\n",
    "\n",
    "            # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.\n",
    "            loss = nn.CrossEntropyLoss()(logit, y)\n",
    "\n",
    "            # flush out the previously computed gradient.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # backward the computed loss. \n",
    "            loss.backward()\n",
    "\n",
    "            # update the network weights. \n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % args.log_iter == 0 and writer is not None:\n",
    "                # Log loss and accuracy values using `writer`. Use `global_step` as a timestamp for the log. \n",
    "                writer.add_scalar('train_loss', loss, global_step)\n",
    "                writer.add_scalar('train_accuracy', accuracy, global_step)\n",
    "\n",
    "            if global_step % args.ckpt_iter == 0: \n",
    "                # Save network weights in the directory specified by `ckpt_dir` directory. \n",
    "                torch.save(net.state_dict(), f'{ckpt_dir}/{global_step}.pt')\n",
    "\n",
    "        # Here starts the test loop.\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.\n",
    "            test_accuracy = 0.\n",
    "            test_num_data = 0.\n",
    "            for batch_idx, (x, y) in enumerate(test_dataloader):\n",
    "                # Send `x` and `y` to either cpu or gpu using `device` variable..\n",
    "                x = x.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "\n",
    "                # Feed `x` into the network, get an output, and keep it in a variable called `logit`.\n",
    "                logit = net(x)\n",
    "\n",
    "                # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.\n",
    "                loss = nn.CrossEntropyLoss()(logit, y)\n",
    "\n",
    "                # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.\n",
    "                accuracy = (logit.argmax(dim=1) == y).float().mean()\n",
    "\n",
    "                test_loss += loss.item()*x.shape[0]\n",
    "                test_accuracy += accuracy.item()*x.shape[0]\n",
    "                test_num_data += x.shape[0]\n",
    "\n",
    "            test_loss /= test_num_data\n",
    "            test_accuracy /= test_num_data\n",
    "\n",
    "            if writer is not None: \n",
    "                # Log loss and accuracy values using `writer`. Use `global_step` as a timestamp for the log. \n",
    "                writer.add_scalar('test_loss', test_loss, global_step)\n",
    "                writer.add_scalar('test_accuracy', test_accuracy, global_step)\n",
    "\n",
    "                # Just for checking progress\n",
    "                print(f'Test result of epoch {epoch}/{args.epoch} || loss : {test_loss:.3f} acc : {test_accuracy:.3f} ')\n",
    "\n",
    "                writer.flush()\n",
    "\n",
    "            # Whenever `test_accuracy` is greater than `best_accuracy`, save network weights with the filename 'best.pt' in the directory specified by `ckpt_dir`.\n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                torch.save(net.state_dict(), f'{ckpt_dir}/{block_type}_best.pt')\n",
    "    \n",
    "        scheduler.step()\n",
    "    return best_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models Through the Pipeline\n",
    "\n",
    "Training a single model for 100 epochs will take around 40~50 minutes. Use this information as an indicator for your experiments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for weight initialization.\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs and ckpts will be saved in : /gdrive/My Drive/Colab Notebooks/CS492I/assignment1/results/trial_5\n",
      "# of parameters in mlp net : 1649354\n",
      "Correct # of parameters in mlp net : 1649354\n",
      "# of parameters in conv net : 510426\n",
      "Correct # of parameters in conv net : 510426\n",
      "# of parameters in resPlain net : 510426\n",
      "Correct # of parameters in resPlain net : 510426\n",
      "# of parameters in resBottleneck net : 113946\n",
      "Correct # of parameters in resBottleneck net : 113946\n",
      "# of parameters in inception net : 124026\n",
      "Correct # of parameters in inception net : 124026\n"
     ]
    }
   ],
   "source": [
    "# List of all block types we will use.\n",
    "block_types = ['mlp', 'conv','resPlain','resBottleneck','inception']\n",
    "\n",
    "# Create directory name.\n",
    "num_trial=0\n",
    "parent_dir = result_dir / f'trial_{num_trial}'\n",
    "while parent_dir.is_dir():\n",
    "    num_trial = int(parent_dir.name.replace('trial_',''))\n",
    "    parent_dir = result_dir / f'trial_{num_trial+1}'\n",
    "print(f'Logs and ckpts will be saved in : {parent_dir}')\n",
    "\n",
    "# Define networks\n",
    "networks = []\n",
    "for block_type in block_types:\n",
    "    if block_type == 'conv':\n",
    "        args.num_blocks = [10, 10, 10]\n",
    "    else:\n",
    "        args.num_blocks = [5, 5, 5]\n",
    "    \n",
    "    if block_type == 'mlp':\n",
    "        network = MyNetworkExample(64, block_type).to(device)\n",
    "    else:\n",
    "        network = MyNetwork(args.num_filters, block_type, args.num_blocks).to(device)\n",
    "\n",
    "    network.apply(weight_init)\n",
    "    networks.append(network)\n",
    "\n",
    "# Count the number of parameters of the models. \n",
    "# You can use it as an indicator of whether you correctly implemented the model.\n",
    "\n",
    "correct_params = {'mlp' : 1649354, 'conv' : 510426, 'resPlain' : 510426, 'resBottleneck' : 113946, 'inception' : 124026}\n",
    "for network, block_type in zip(networks, block_types):\n",
    "    # Print the number of parameters in each model.\n",
    "    num_parameters = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "    print(f'# of parameters in {block_type} net : {num_parameters}')\n",
    "    print(f'Correct # of parameters in {block_type} net : {correct_params[block_type]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result of epoch 0/100 || loss : 1.675 acc : 0.397 \n",
      "Test result of epoch 1/100 || loss : 1.526 acc : 0.447 \n",
      "Test result of epoch 2/100 || loss : 1.456 acc : 0.474 \n",
      "Test result of epoch 3/100 || loss : 1.430 acc : 0.482 \n",
      "Test result of epoch 4/100 || loss : 1.403 acc : 0.498 \n",
      "Test result of epoch 5/100 || loss : 1.392 acc : 0.499 \n",
      "Test result of epoch 6/100 || loss : 1.391 acc : 0.505 \n",
      "Test result of epoch 7/100 || loss : 1.355 acc : 0.516 \n",
      "Test result of epoch 8/100 || loss : 1.347 acc : 0.515 \n",
      "Test result of epoch 9/100 || loss : 1.312 acc : 0.526 \n",
      "Test result of epoch 10/100 || loss : 1.314 acc : 0.530 \n",
      "Test result of epoch 11/100 || loss : 1.299 acc : 0.535 \n",
      "Test result of epoch 12/100 || loss : 1.316 acc : 0.530 \n",
      "Test result of epoch 13/100 || loss : 1.299 acc : 0.536 \n",
      "Test result of epoch 14/100 || loss : 1.285 acc : 0.537 \n",
      "Test result of epoch 15/100 || loss : 1.259 acc : 0.540 \n",
      "Test result of epoch 16/100 || loss : 1.267 acc : 0.543 \n",
      "Test result of epoch 17/100 || loss : 1.296 acc : 0.529 \n",
      "Test result of epoch 18/100 || loss : 1.271 acc : 0.545 \n",
      "Test result of epoch 19/100 || loss : 1.263 acc : 0.546 \n",
      "Test result of epoch 20/100 || loss : 1.235 acc : 0.558 \n",
      "Test result of epoch 21/100 || loss : 1.265 acc : 0.548 \n",
      "Test result of epoch 22/100 || loss : 1.250 acc : 0.557 \n",
      "Test result of epoch 23/100 || loss : 1.255 acc : 0.553 \n",
      "Test result of epoch 24/100 || loss : 1.239 acc : 0.554 \n",
      "Test result of epoch 25/100 || loss : 1.251 acc : 0.552 \n",
      "Test result of epoch 26/100 || loss : 1.239 acc : 0.558 \n",
      "Test result of epoch 27/100 || loss : 1.229 acc : 0.558 \n",
      "Test result of epoch 28/100 || loss : 1.231 acc : 0.558 \n",
      "Test result of epoch 29/100 || loss : 1.249 acc : 0.553 \n",
      "Test result of epoch 30/100 || loss : 1.249 acc : 0.559 \n",
      "Test result of epoch 31/100 || loss : 1.234 acc : 0.563 \n",
      "Test result of epoch 32/100 || loss : 1.215 acc : 0.564 \n",
      "Test result of epoch 33/100 || loss : 1.261 acc : 0.551 \n",
      "Test result of epoch 34/100 || loss : 1.225 acc : 0.555 \n",
      "Test result of epoch 35/100 || loss : 1.205 acc : 0.564 \n",
      "Test result of epoch 36/100 || loss : 1.214 acc : 0.564 \n",
      "Test result of epoch 37/100 || loss : 1.227 acc : 0.567 \n",
      "Test result of epoch 38/100 || loss : 1.209 acc : 0.567 \n",
      "Test result of epoch 39/100 || loss : 1.215 acc : 0.564 \n",
      "Test result of epoch 40/100 || loss : 1.221 acc : 0.564 \n",
      "Test result of epoch 41/100 || loss : 1.215 acc : 0.564 \n",
      "Test result of epoch 42/100 || loss : 1.215 acc : 0.563 \n",
      "Test result of epoch 43/100 || loss : 1.222 acc : 0.567 \n",
      "Test result of epoch 44/100 || loss : 1.194 acc : 0.571 \n",
      "Test result of epoch 45/100 || loss : 1.199 acc : 0.571 \n",
      "Test result of epoch 46/100 || loss : 1.196 acc : 0.574 \n",
      "Test result of epoch 47/100 || loss : 1.200 acc : 0.568 \n",
      "Test result of epoch 48/100 || loss : 1.192 acc : 0.576 \n",
      "Test result of epoch 49/100 || loss : 1.199 acc : 0.573 \n",
      "Test result of epoch 50/100 || loss : 1.144 acc : 0.591 \n",
      "Test result of epoch 51/100 || loss : 1.138 acc : 0.598 \n",
      "Test result of epoch 52/100 || loss : 1.144 acc : 0.595 \n",
      "Test result of epoch 53/100 || loss : 1.143 acc : 0.595 \n",
      "Test result of epoch 54/100 || loss : 1.129 acc : 0.601 \n",
      "Test result of epoch 55/100 || loss : 1.147 acc : 0.591 \n",
      "Test result of epoch 56/100 || loss : 1.129 acc : 0.599 \n",
      "Test result of epoch 57/100 || loss : 1.132 acc : 0.600 \n",
      "Test result of epoch 58/100 || loss : 1.141 acc : 0.595 \n",
      "Test result of epoch 59/100 || loss : 1.134 acc : 0.599 \n",
      "Test result of epoch 60/100 || loss : 1.138 acc : 0.593 \n",
      "Test result of epoch 61/100 || loss : 1.134 acc : 0.598 \n",
      "Test result of epoch 62/100 || loss : 1.129 acc : 0.598 \n",
      "Test result of epoch 63/100 || loss : 1.138 acc : 0.603 \n",
      "Test result of epoch 64/100 || loss : 1.139 acc : 0.594 \n",
      "Test result of epoch 65/100 || loss : 1.133 acc : 0.592 \n",
      "Test result of epoch 66/100 || loss : 1.149 acc : 0.595 \n",
      "Test result of epoch 67/100 || loss : 1.142 acc : 0.593 \n",
      "Test result of epoch 68/100 || loss : 1.117 acc : 0.606 \n",
      "Test result of epoch 69/100 || loss : 1.116 acc : 0.611 \n",
      "Test result of epoch 70/100 || loss : 1.142 acc : 0.593 \n",
      "Test result of epoch 71/100 || loss : 1.150 acc : 0.594 \n",
      "Test result of epoch 72/100 || loss : 1.128 acc : 0.601 \n",
      "Test result of epoch 73/100 || loss : 1.137 acc : 0.598 \n",
      "Test result of epoch 74/100 || loss : 1.136 acc : 0.597 \n",
      "Test result of epoch 75/100 || loss : 1.133 acc : 0.599 \n",
      "Test result of epoch 76/100 || loss : 1.129 acc : 0.600 \n",
      "Test result of epoch 77/100 || loss : 1.128 acc : 0.603 \n",
      "Test result of epoch 78/100 || loss : 1.118 acc : 0.606 \n",
      "Test result of epoch 79/100 || loss : 1.124 acc : 0.603 \n",
      "Test result of epoch 80/100 || loss : 1.089 acc : 0.618 \n",
      "Test result of epoch 81/100 || loss : 1.082 acc : 0.618 \n",
      "Test result of epoch 82/100 || loss : 1.089 acc : 0.618 \n",
      "Test result of epoch 83/100 || loss : 1.086 acc : 0.614 \n",
      "Test result of epoch 84/100 || loss : 1.093 acc : 0.615 \n",
      "Test result of epoch 85/100 || loss : 1.079 acc : 0.620 \n",
      "Test result of epoch 86/100 || loss : 1.081 acc : 0.617 \n",
      "Test result of epoch 87/100 || loss : 1.083 acc : 0.618 \n",
      "Test result of epoch 88/100 || loss : 1.077 acc : 0.621 \n",
      "Test result of epoch 89/100 || loss : 1.077 acc : 0.623 \n",
      "Test result of epoch 90/100 || loss : 1.068 acc : 0.621 \n",
      "Test result of epoch 91/100 || loss : 1.081 acc : 0.618 \n",
      "Test result of epoch 92/100 || loss : 1.081 acc : 0.622 \n",
      "Test result of epoch 93/100 || loss : 1.070 acc : 0.623 \n",
      "Test result of epoch 94/100 || loss : 1.066 acc : 0.624 \n",
      "Test result of epoch 95/100 || loss : 1.090 acc : 0.617 \n",
      "Test result of epoch 96/100 || loss : 1.087 acc : 0.614 \n",
      "Test result of epoch 97/100 || loss : 1.093 acc : 0.617 \n",
      "Test result of epoch 98/100 || loss : 1.074 acc : 0.621 \n",
      "Test result of epoch 99/100 || loss : 1.079 acc : 0.624 \n",
      "Best test accuracy of mlp network : 0.624 took 1445.924 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result of epoch 0/100 || loss : 1.813 acc : 0.296 \n",
      "Test result of epoch 1/100 || loss : 1.699 acc : 0.365 \n",
      "Test result of epoch 2/100 || loss : 1.606 acc : 0.395 \n",
      "Test result of epoch 3/100 || loss : 1.580 acc : 0.415 \n",
      "Test result of epoch 4/100 || loss : 1.421 acc : 0.470 \n",
      "Test result of epoch 5/100 || loss : 1.405 acc : 0.493 \n",
      "Test result of epoch 6/100 || loss : 1.336 acc : 0.510 \n",
      "Test result of epoch 7/100 || loss : 1.376 acc : 0.511 \n",
      "Test result of epoch 8/100 || loss : 1.599 acc : 0.454 \n",
      "Test result of epoch 9/100 || loss : 1.298 acc : 0.545 \n",
      "Test result of epoch 10/100 || loss : 1.310 acc : 0.538 \n",
      "Test result of epoch 11/100 || loss : 1.224 acc : 0.560 \n",
      "Test result of epoch 12/100 || loss : 1.213 acc : 0.583 \n",
      "Test result of epoch 13/100 || loss : 1.202 acc : 0.567 \n",
      "Test result of epoch 14/100 || loss : 1.268 acc : 0.562 \n",
      "Test result of epoch 15/100 || loss : 1.347 acc : 0.549 \n",
      "Test result of epoch 16/100 || loss : 1.130 acc : 0.608 \n",
      "Test result of epoch 17/100 || loss : 1.307 acc : 0.562 \n",
      "Test result of epoch 18/100 || loss : 1.088 acc : 0.624 \n",
      "Test result of epoch 19/100 || loss : 1.100 acc : 0.625 \n",
      "Test result of epoch 20/100 || loss : 1.341 acc : 0.550 \n",
      "Test result of epoch 21/100 || loss : 1.114 acc : 0.619 \n",
      "Test result of epoch 22/100 || loss : 1.361 acc : 0.553 \n",
      "Test result of epoch 23/100 || loss : 0.967 acc : 0.667 \n",
      "Test result of epoch 24/100 || loss : 1.114 acc : 0.625 \n",
      "Test result of epoch 25/100 || loss : 1.089 acc : 0.632 \n",
      "Test result of epoch 26/100 || loss : 1.189 acc : 0.596 \n",
      "Test result of epoch 27/100 || loss : 1.041 acc : 0.651 \n",
      "Test result of epoch 28/100 || loss : 0.964 acc : 0.663 \n",
      "Test result of epoch 29/100 || loss : 1.139 acc : 0.603 \n",
      "Test result of epoch 30/100 || loss : 1.109 acc : 0.638 \n",
      "Test result of epoch 31/100 || loss : 1.727 acc : 0.541 \n",
      "Test result of epoch 32/100 || loss : 1.128 acc : 0.625 \n",
      "Test result of epoch 33/100 || loss : 1.143 acc : 0.638 \n",
      "Test result of epoch 34/100 || loss : 0.912 acc : 0.694 \n",
      "Test result of epoch 35/100 || loss : 1.150 acc : 0.625 \n",
      "Test result of epoch 36/100 || loss : 0.822 acc : 0.721 \n",
      "Test result of epoch 37/100 || loss : 1.304 acc : 0.613 \n",
      "Test result of epoch 38/100 || loss : 1.091 acc : 0.653 \n",
      "Test result of epoch 39/100 || loss : 0.987 acc : 0.680 \n",
      "Test result of epoch 40/100 || loss : 0.935 acc : 0.684 \n",
      "Test result of epoch 41/100 || loss : 0.992 acc : 0.677 \n",
      "Test result of epoch 42/100 || loss : 0.921 acc : 0.706 \n",
      "Test result of epoch 43/100 || loss : 0.930 acc : 0.691 \n",
      "Test result of epoch 44/100 || loss : 0.962 acc : 0.678 \n",
      "Test result of epoch 45/100 || loss : 1.256 acc : 0.611 \n",
      "Test result of epoch 46/100 || loss : 1.052 acc : 0.664 \n",
      "Test result of epoch 47/100 || loss : 1.271 acc : 0.608 \n",
      "Test result of epoch 48/100 || loss : 1.128 acc : 0.644 \n",
      "Test result of epoch 49/100 || loss : 0.867 acc : 0.707 \n",
      "Test result of epoch 50/100 || loss : 0.641 acc : 0.784 \n",
      "Test result of epoch 51/100 || loss : 0.654 acc : 0.779 \n",
      "Test result of epoch 52/100 || loss : 0.697 acc : 0.771 \n",
      "Test result of epoch 53/100 || loss : 0.625 acc : 0.786 \n",
      "Test result of epoch 54/100 || loss : 0.692 acc : 0.771 \n",
      "Test result of epoch 55/100 || loss : 0.613 acc : 0.794 \n",
      "Test result of epoch 56/100 || loss : 0.711 acc : 0.759 \n",
      "Test result of epoch 57/100 || loss : 0.813 acc : 0.735 \n",
      "Test result of epoch 58/100 || loss : 0.619 acc : 0.787 \n",
      "Test result of epoch 59/100 || loss : 0.712 acc : 0.761 \n",
      "Test result of epoch 60/100 || loss : 0.928 acc : 0.699 \n",
      "Test result of epoch 61/100 || loss : 0.696 acc : 0.766 \n",
      "Test result of epoch 62/100 || loss : 0.858 acc : 0.723 \n",
      "Test result of epoch 63/100 || loss : 0.641 acc : 0.782 \n",
      "Test result of epoch 64/100 || loss : 0.656 acc : 0.780 \n",
      "Test result of epoch 65/100 || loss : 0.674 acc : 0.775 \n",
      "Test result of epoch 66/100 || loss : 0.662 acc : 0.781 \n",
      "Test result of epoch 67/100 || loss : 0.716 acc : 0.766 \n",
      "Test result of epoch 68/100 || loss : 0.739 acc : 0.759 \n",
      "Test result of epoch 69/100 || loss : 0.719 acc : 0.767 \n",
      "Test result of epoch 70/100 || loss : 0.704 acc : 0.769 \n",
      "Test result of epoch 71/100 || loss : 0.657 acc : 0.783 \n",
      "Test result of epoch 72/100 || loss : 0.914 acc : 0.711 \n",
      "Test result of epoch 73/100 || loss : 0.708 acc : 0.763 \n",
      "Test result of epoch 74/100 || loss : 0.931 acc : 0.702 \n",
      "Test result of epoch 75/100 || loss : 0.665 acc : 0.777 \n",
      "Test result of epoch 76/100 || loss : 0.639 acc : 0.792 \n",
      "Test result of epoch 77/100 || loss : 0.625 acc : 0.788 \n",
      "Test result of epoch 78/100 || loss : 0.693 acc : 0.768 \n",
      "Test result of epoch 79/100 || loss : 0.673 acc : 0.773 \n",
      "Test result of epoch 80/100 || loss : 0.561 acc : 0.809 \n",
      "Test result of epoch 81/100 || loss : 0.538 acc : 0.820 \n",
      "Test result of epoch 82/100 || loss : 0.554 acc : 0.816 \n",
      "Test result of epoch 83/100 || loss : 0.564 acc : 0.810 \n",
      "Test result of epoch 84/100 || loss : 0.561 acc : 0.816 \n",
      "Test result of epoch 85/100 || loss : 0.543 acc : 0.817 \n",
      "Test result of epoch 86/100 || loss : 0.587 acc : 0.803 \n",
      "Test result of epoch 87/100 || loss : 0.547 acc : 0.820 \n",
      "Test result of epoch 88/100 || loss : 0.560 acc : 0.818 \n",
      "Test result of epoch 89/100 || loss : 0.577 acc : 0.813 \n",
      "Test result of epoch 90/100 || loss : 0.578 acc : 0.808 \n",
      "Test result of epoch 91/100 || loss : 0.553 acc : 0.814 \n",
      "Test result of epoch 92/100 || loss : 0.613 acc : 0.801 \n",
      "Test result of epoch 93/100 || loss : 0.535 acc : 0.822 \n",
      "Test result of epoch 94/100 || loss : 0.537 acc : 0.822 \n",
      "Test result of epoch 95/100 || loss : 0.549 acc : 0.817 \n",
      "Test result of epoch 96/100 || loss : 0.540 acc : 0.822 \n",
      "Test result of epoch 97/100 || loss : 0.545 acc : 0.819 \n",
      "Test result of epoch 98/100 || loss : 0.607 acc : 0.802 \n",
      "Test result of epoch 99/100 || loss : 0.549 acc : 0.817 \n",
      "Best test accuracy of conv network : 0.822 took 1877.277 secs\n",
      "Test result of epoch 0/100 || loss : 1.511 acc : 0.411 \n",
      "Test result of epoch 1/100 || loss : 1.326 acc : 0.517 \n",
      "Test result of epoch 2/100 || loss : 1.138 acc : 0.593 \n",
      "Test result of epoch 3/100 || loss : 1.294 acc : 0.569 \n",
      "Test result of epoch 4/100 || loss : 1.042 acc : 0.635 \n",
      "Test result of epoch 5/100 || loss : 1.022 acc : 0.657 \n",
      "Test result of epoch 6/100 || loss : 0.921 acc : 0.684 \n",
      "Test result of epoch 7/100 || loss : 0.790 acc : 0.719 \n",
      "Test result of epoch 8/100 || loss : 0.717 acc : 0.753 \n",
      "Test result of epoch 9/100 || loss : 0.828 acc : 0.707 \n",
      "Test result of epoch 10/100 || loss : 0.725 acc : 0.749 \n",
      "Test result of epoch 11/100 || loss : 0.736 acc : 0.756 \n",
      "Test result of epoch 12/100 || loss : 0.682 acc : 0.766 \n",
      "Test result of epoch 13/100 || loss : 0.749 acc : 0.742 \n",
      "Test result of epoch 14/100 || loss : 0.675 acc : 0.771 \n",
      "Test result of epoch 15/100 || loss : 0.602 acc : 0.794 \n",
      "Test result of epoch 16/100 || loss : 0.794 acc : 0.751 \n",
      "Test result of epoch 17/100 || loss : 0.570 acc : 0.804 \n",
      "Test result of epoch 18/100 || loss : 0.583 acc : 0.803 \n",
      "Test result of epoch 19/100 || loss : 0.597 acc : 0.804 \n",
      "Test result of epoch 20/100 || loss : 0.536 acc : 0.818 \n",
      "Test result of epoch 21/100 || loss : 0.603 acc : 0.799 \n",
      "Test result of epoch 22/100 || loss : 0.743 acc : 0.757 \n",
      "Test result of epoch 23/100 || loss : 0.547 acc : 0.815 \n",
      "Test result of epoch 24/100 || loss : 0.718 acc : 0.777 \n",
      "Test result of epoch 25/100 || loss : 0.516 acc : 0.824 \n",
      "Test result of epoch 26/100 || loss : 0.551 acc : 0.813 \n",
      "Test result of epoch 27/100 || loss : 0.551 acc : 0.817 \n",
      "Test result of epoch 28/100 || loss : 0.539 acc : 0.817 \n",
      "Test result of epoch 29/100 || loss : 0.615 acc : 0.806 \n",
      "Test result of epoch 30/100 || loss : 0.758 acc : 0.770 \n",
      "Test result of epoch 31/100 || loss : 0.536 acc : 0.824 \n",
      "Test result of epoch 32/100 || loss : 0.458 acc : 0.845 \n",
      "Test result of epoch 33/100 || loss : 0.528 acc : 0.824 \n",
      "Test result of epoch 34/100 || loss : 0.556 acc : 0.816 \n",
      "Test result of epoch 35/100 || loss : 0.618 acc : 0.803 \n",
      "Test result of epoch 36/100 || loss : 0.515 acc : 0.830 \n",
      "Test result of epoch 37/100 || loss : 0.550 acc : 0.817 \n",
      "Test result of epoch 38/100 || loss : 0.538 acc : 0.823 \n",
      "Test result of epoch 39/100 || loss : 0.544 acc : 0.819 \n",
      "Test result of epoch 40/100 || loss : 0.568 acc : 0.812 \n",
      "Test result of epoch 41/100 || loss : 0.612 acc : 0.813 \n",
      "Test result of epoch 42/100 || loss : 0.600 acc : 0.808 \n",
      "Test result of epoch 43/100 || loss : 0.507 acc : 0.836 \n",
      "Test result of epoch 44/100 || loss : 0.574 acc : 0.822 \n",
      "Test result of epoch 45/100 || loss : 0.511 acc : 0.834 \n",
      "Test result of epoch 46/100 || loss : 0.579 acc : 0.814 \n",
      "Test result of epoch 47/100 || loss : 0.494 acc : 0.838 \n",
      "Test result of epoch 48/100 || loss : 0.451 acc : 0.848 \n",
      "Test result of epoch 49/100 || loss : 0.506 acc : 0.837 \n",
      "Test result of epoch 50/100 || loss : 0.407 acc : 0.867 \n",
      "Test result of epoch 51/100 || loss : 0.403 acc : 0.869 \n",
      "Test result of epoch 52/100 || loss : 0.439 acc : 0.862 \n",
      "Test result of epoch 53/100 || loss : 0.393 acc : 0.872 \n",
      "Test result of epoch 54/100 || loss : 0.429 acc : 0.864 \n",
      "Test result of epoch 55/100 || loss : 0.456 acc : 0.855 \n",
      "Test result of epoch 56/100 || loss : 0.498 acc : 0.851 \n",
      "Test result of epoch 57/100 || loss : 0.407 acc : 0.868 \n",
      "Test result of epoch 58/100 || loss : 0.440 acc : 0.863 \n",
      "Test result of epoch 59/100 || loss : 0.430 acc : 0.862 \n",
      "Test result of epoch 60/100 || loss : 0.441 acc : 0.863 \n",
      "Test result of epoch 61/100 || loss : 0.421 acc : 0.869 \n",
      "Test result of epoch 62/100 || loss : 0.446 acc : 0.860 \n",
      "Test result of epoch 63/100 || loss : 0.499 acc : 0.848 \n",
      "Test result of epoch 64/100 || loss : 0.466 acc : 0.859 \n",
      "Test result of epoch 65/100 || loss : 0.514 acc : 0.845 \n",
      "Test result of epoch 66/100 || loss : 0.449 acc : 0.861 \n",
      "Test result of epoch 67/100 || loss : 0.450 acc : 0.858 \n",
      "Test result of epoch 68/100 || loss : 0.460 acc : 0.854 \n",
      "Test result of epoch 69/100 || loss : 0.439 acc : 0.862 \n",
      "Test result of epoch 70/100 || loss : 0.447 acc : 0.860 \n",
      "Test result of epoch 71/100 || loss : 0.441 acc : 0.866 \n",
      "Test result of epoch 72/100 || loss : 0.421 acc : 0.871 \n",
      "Test result of epoch 73/100 || loss : 0.513 acc : 0.850 \n",
      "Test result of epoch 74/100 || loss : 0.426 acc : 0.868 \n",
      "Test result of epoch 75/100 || loss : 0.416 acc : 0.870 \n",
      "Test result of epoch 76/100 || loss : 0.414 acc : 0.873 \n",
      "Test result of epoch 77/100 || loss : 0.444 acc : 0.864 \n",
      "Test result of epoch 78/100 || loss : 0.457 acc : 0.857 \n",
      "Test result of epoch 79/100 || loss : 0.448 acc : 0.862 \n",
      "Test result of epoch 80/100 || loss : 0.384 acc : 0.885 \n",
      "Test result of epoch 81/100 || loss : 0.382 acc : 0.888 \n",
      "Test result of epoch 82/100 || loss : 0.435 acc : 0.871 \n",
      "Test result of epoch 83/100 || loss : 0.390 acc : 0.887 \n",
      "Test result of epoch 84/100 || loss : 0.403 acc : 0.884 \n",
      "Test result of epoch 85/100 || loss : 0.397 acc : 0.885 \n",
      "Test result of epoch 86/100 || loss : 0.422 acc : 0.882 \n",
      "Test result of epoch 87/100 || loss : 0.430 acc : 0.882 \n",
      "Test result of epoch 88/100 || loss : 0.437 acc : 0.882 \n",
      "Test result of epoch 89/100 || loss : 0.402 acc : 0.886 \n",
      "Test result of epoch 90/100 || loss : 0.412 acc : 0.885 \n",
      "Test result of epoch 91/100 || loss : 0.414 acc : 0.883 \n",
      "Test result of epoch 92/100 || loss : 0.444 acc : 0.876 \n",
      "Test result of epoch 93/100 || loss : 0.427 acc : 0.877 \n",
      "Test result of epoch 94/100 || loss : 0.430 acc : 0.876 \n",
      "Test result of epoch 95/100 || loss : 0.426 acc : 0.883 \n",
      "Test result of epoch 96/100 || loss : 0.421 acc : 0.879 \n",
      "Test result of epoch 97/100 || loss : 0.424 acc : 0.881 \n",
      "Test result of epoch 98/100 || loss : 0.426 acc : 0.878 \n",
      "Test result of epoch 99/100 || loss : 0.442 acc : 0.876 \n",
      "Best test accuracy of resPlain network : 0.888 took 1890.309 secs\n",
      "Test result of epoch 0/100 || loss : 1.813 acc : 0.375 \n",
      "Test result of epoch 1/100 || loss : 1.418 acc : 0.487 \n",
      "Test result of epoch 2/100 || loss : 1.359 acc : 0.527 \n",
      "Test result of epoch 3/100 || loss : 1.251 acc : 0.561 \n",
      "Test result of epoch 4/100 || loss : 1.047 acc : 0.636 \n",
      "Test result of epoch 5/100 || loss : 1.300 acc : 0.560 \n",
      "Test result of epoch 6/100 || loss : 0.974 acc : 0.662 \n",
      "Test result of epoch 7/100 || loss : 0.989 acc : 0.671 \n",
      "Test result of epoch 8/100 || loss : 0.840 acc : 0.715 \n",
      "Test result of epoch 9/100 || loss : 0.820 acc : 0.715 \n",
      "Test result of epoch 10/100 || loss : 0.797 acc : 0.719 \n",
      "Test result of epoch 11/100 || loss : 0.865 acc : 0.724 \n",
      "Test result of epoch 12/100 || loss : 0.777 acc : 0.734 \n",
      "Test result of epoch 13/100 || loss : 0.791 acc : 0.732 \n",
      "Test result of epoch 14/100 || loss : 0.717 acc : 0.759 \n",
      "Test result of epoch 15/100 || loss : 0.718 acc : 0.761 \n",
      "Test result of epoch 16/100 || loss : 0.700 acc : 0.762 \n",
      "Test result of epoch 17/100 || loss : 0.843 acc : 0.721 \n",
      "Test result of epoch 18/100 || loss : 0.683 acc : 0.766 \n",
      "Test result of epoch 19/100 || loss : 0.657 acc : 0.781 \n",
      "Test result of epoch 20/100 || loss : 0.722 acc : 0.755 \n",
      "Test result of epoch 21/100 || loss : 0.704 acc : 0.761 \n",
      "Test result of epoch 22/100 || loss : 0.671 acc : 0.773 \n",
      "Test result of epoch 23/100 || loss : 0.747 acc : 0.754 \n",
      "Test result of epoch 24/100 || loss : 0.686 acc : 0.769 \n",
      "Test result of epoch 25/100 || loss : 0.737 acc : 0.753 \n",
      "Test result of epoch 26/100 || loss : 0.730 acc : 0.756 \n",
      "Test result of epoch 27/100 || loss : 0.639 acc : 0.783 \n",
      "Test result of epoch 28/100 || loss : 0.602 acc : 0.792 \n",
      "Test result of epoch 29/100 || loss : 0.767 acc : 0.757 \n",
      "Test result of epoch 30/100 || loss : 0.635 acc : 0.782 \n",
      "Test result of epoch 31/100 || loss : 0.671 acc : 0.776 \n",
      "Test result of epoch 32/100 || loss : 0.647 acc : 0.777 \n",
      "Test result of epoch 33/100 || loss : 0.595 acc : 0.798 \n",
      "Test result of epoch 34/100 || loss : 0.630 acc : 0.790 \n",
      "Test result of epoch 35/100 || loss : 0.590 acc : 0.798 \n",
      "Test result of epoch 36/100 || loss : 0.618 acc : 0.794 \n",
      "Test result of epoch 37/100 || loss : 0.561 acc : 0.810 \n",
      "Test result of epoch 38/100 || loss : 0.876 acc : 0.728 \n",
      "Test result of epoch 39/100 || loss : 0.639 acc : 0.781 \n",
      "Test result of epoch 40/100 || loss : 0.644 acc : 0.785 \n",
      "Test result of epoch 41/100 || loss : 0.595 acc : 0.802 \n",
      "Test result of epoch 42/100 || loss : 0.589 acc : 0.807 \n",
      "Test result of epoch 43/100 || loss : 0.594 acc : 0.803 \n",
      "Test result of epoch 44/100 || loss : 0.629 acc : 0.789 \n",
      "Test result of epoch 45/100 || loss : 0.589 acc : 0.797 \n",
      "Test result of epoch 46/100 || loss : 0.581 acc : 0.805 \n",
      "Test result of epoch 47/100 || loss : 0.621 acc : 0.796 \n",
      "Test result of epoch 48/100 || loss : 0.660 acc : 0.781 \n",
      "Test result of epoch 49/100 || loss : 0.565 acc : 0.813 \n",
      "Test result of epoch 50/100 || loss : 0.456 acc : 0.845 \n",
      "Test result of epoch 51/100 || loss : 0.515 acc : 0.830 \n",
      "Test result of epoch 52/100 || loss : 0.468 acc : 0.843 \n",
      "Test result of epoch 53/100 || loss : 0.470 acc : 0.845 \n",
      "Test result of epoch 54/100 || loss : 0.501 acc : 0.833 \n",
      "Test result of epoch 55/100 || loss : 0.489 acc : 0.839 \n",
      "Test result of epoch 56/100 || loss : 0.492 acc : 0.835 \n",
      "Test result of epoch 57/100 || loss : 0.465 acc : 0.841 \n",
      "Test result of epoch 58/100 || loss : 0.491 acc : 0.836 \n",
      "Test result of epoch 59/100 || loss : 0.489 acc : 0.840 \n",
      "Test result of epoch 60/100 || loss : 0.505 acc : 0.834 \n",
      "Test result of epoch 61/100 || loss : 0.453 acc : 0.847 \n",
      "Test result of epoch 62/100 || loss : 0.490 acc : 0.837 \n",
      "Test result of epoch 63/100 || loss : 0.499 acc : 0.835 \n",
      "Test result of epoch 64/100 || loss : 0.476 acc : 0.839 \n",
      "Test result of epoch 65/100 || loss : 0.461 acc : 0.847 \n",
      "Test result of epoch 66/100 || loss : 0.480 acc : 0.839 \n",
      "Test result of epoch 67/100 || loss : 0.485 acc : 0.837 \n",
      "Test result of epoch 68/100 || loss : 0.506 acc : 0.835 \n",
      "Test result of epoch 69/100 || loss : 0.510 acc : 0.828 \n",
      "Test result of epoch 70/100 || loss : 0.505 acc : 0.834 \n",
      "Test result of epoch 71/100 || loss : 0.468 acc : 0.845 \n",
      "Test result of epoch 72/100 || loss : 0.513 acc : 0.829 \n",
      "Test result of epoch 73/100 || loss : 0.498 acc : 0.836 \n",
      "Test result of epoch 74/100 || loss : 0.467 acc : 0.841 \n",
      "Test result of epoch 75/100 || loss : 0.478 acc : 0.844 \n",
      "Test result of epoch 76/100 || loss : 0.511 acc : 0.828 \n",
      "Test result of epoch 77/100 || loss : 0.505 acc : 0.832 \n",
      "Test result of epoch 78/100 || loss : 0.494 acc : 0.836 \n",
      "Test result of epoch 79/100 || loss : 0.494 acc : 0.835 \n",
      "Test result of epoch 80/100 || loss : 0.432 acc : 0.858 \n",
      "Test result of epoch 81/100 || loss : 0.433 acc : 0.859 \n",
      "Test result of epoch 82/100 || loss : 0.439 acc : 0.858 \n",
      "Test result of epoch 83/100 || loss : 0.433 acc : 0.861 \n",
      "Test result of epoch 84/100 || loss : 0.459 acc : 0.854 \n",
      "Test result of epoch 85/100 || loss : 0.421 acc : 0.867 \n",
      "Test result of epoch 86/100 || loss : 0.432 acc : 0.863 \n",
      "Test result of epoch 87/100 || loss : 0.439 acc : 0.859 \n",
      "Test result of epoch 88/100 || loss : 0.440 acc : 0.858 \n",
      "Test result of epoch 89/100 || loss : 0.448 acc : 0.853 \n",
      "Test result of epoch 90/100 || loss : 0.437 acc : 0.858 \n",
      "Test result of epoch 91/100 || loss : 0.446 acc : 0.856 \n",
      "Test result of epoch 92/100 || loss : 0.443 acc : 0.857 \n",
      "Test result of epoch 93/100 || loss : 0.461 acc : 0.853 \n",
      "Test result of epoch 94/100 || loss : 0.450 acc : 0.859 \n",
      "Test result of epoch 95/100 || loss : 0.473 acc : 0.853 \n",
      "Test result of epoch 96/100 || loss : 0.454 acc : 0.851 \n",
      "Test result of epoch 97/100 || loss : 0.439 acc : 0.858 \n",
      "Test result of epoch 98/100 || loss : 0.432 acc : 0.864 \n",
      "Test result of epoch 99/100 || loss : 0.497 acc : 0.840 \n",
      "Best test accuracy of resBottleneck network : 0.867 took 2115.863 secs\n",
      "Test result of epoch 0/100 || loss : 1.757 acc : 0.354 \n",
      "Test result of epoch 1/100 || loss : 1.548 acc : 0.427 \n",
      "Test result of epoch 2/100 || loss : 1.442 acc : 0.478 \n",
      "Test result of epoch 3/100 || loss : 1.400 acc : 0.496 \n",
      "Test result of epoch 4/100 || loss : 1.299 acc : 0.530 \n",
      "Test result of epoch 5/100 || loss : 1.262 acc : 0.543 \n",
      "Test result of epoch 6/100 || loss : 1.272 acc : 0.545 \n",
      "Test result of epoch 7/100 || loss : 1.131 acc : 0.598 \n",
      "Test result of epoch 8/100 || loss : 1.100 acc : 0.612 \n",
      "Test result of epoch 9/100 || loss : 1.124 acc : 0.603 \n",
      "Test result of epoch 10/100 || loss : 1.067 acc : 0.618 \n",
      "Test result of epoch 11/100 || loss : 1.092 acc : 0.616 \n",
      "Test result of epoch 12/100 || loss : 1.041 acc : 0.626 \n",
      "Test result of epoch 13/100 || loss : 1.037 acc : 0.642 \n",
      "Test result of epoch 14/100 || loss : 0.975 acc : 0.651 \n",
      "Test result of epoch 15/100 || loss : 1.015 acc : 0.644 \n",
      "Test result of epoch 16/100 || loss : 1.032 acc : 0.639 \n",
      "Test result of epoch 17/100 || loss : 1.044 acc : 0.638 \n",
      "Test result of epoch 18/100 || loss : 0.929 acc : 0.679 \n",
      "Test result of epoch 19/100 || loss : 0.966 acc : 0.668 \n",
      "Test result of epoch 20/100 || loss : 0.841 acc : 0.704 \n",
      "Test result of epoch 21/100 || loss : 1.023 acc : 0.655 \n",
      "Test result of epoch 22/100 || loss : 0.868 acc : 0.696 \n",
      "Test result of epoch 23/100 || loss : 0.866 acc : 0.698 \n",
      "Test result of epoch 24/100 || loss : 0.866 acc : 0.702 \n",
      "Test result of epoch 25/100 || loss : 0.918 acc : 0.687 \n",
      "Test result of epoch 26/100 || loss : 0.893 acc : 0.699 \n",
      "Test result of epoch 27/100 || loss : 0.809 acc : 0.718 \n",
      "Test result of epoch 28/100 || loss : 0.893 acc : 0.695 \n",
      "Test result of epoch 29/100 || loss : 0.832 acc : 0.720 \n",
      "Test result of epoch 30/100 || loss : 0.782 acc : 0.729 \n",
      "Test result of epoch 31/100 || loss : 0.852 acc : 0.700 \n",
      "Test result of epoch 32/100 || loss : 0.805 acc : 0.726 \n",
      "Test result of epoch 33/100 || loss : 0.821 acc : 0.717 \n",
      "Test result of epoch 34/100 || loss : 0.765 acc : 0.728 \n",
      "Test result of epoch 35/100 || loss : 0.856 acc : 0.715 \n",
      "Test result of epoch 36/100 || loss : 0.751 acc : 0.743 \n",
      "Test result of epoch 37/100 || loss : 0.768 acc : 0.741 \n",
      "Test result of epoch 38/100 || loss : 1.050 acc : 0.668 \n",
      "Test result of epoch 39/100 || loss : 0.840 acc : 0.721 \n",
      "Test result of epoch 40/100 || loss : 0.951 acc : 0.688 \n",
      "Test result of epoch 41/100 || loss : 0.723 acc : 0.747 \n",
      "Test result of epoch 42/100 || loss : 0.737 acc : 0.746 \n",
      "Test result of epoch 43/100 || loss : 0.695 acc : 0.758 \n",
      "Test result of epoch 44/100 || loss : 0.738 acc : 0.755 \n",
      "Test result of epoch 45/100 || loss : 0.752 acc : 0.743 \n",
      "Test result of epoch 46/100 || loss : 0.668 acc : 0.762 \n",
      "Test result of epoch 47/100 || loss : 0.741 acc : 0.751 \n",
      "Test result of epoch 48/100 || loss : 0.655 acc : 0.774 \n",
      "Test result of epoch 49/100 || loss : 0.819 acc : 0.730 \n",
      "Test result of epoch 50/100 || loss : 0.585 acc : 0.804 \n",
      "Test result of epoch 51/100 || loss : 0.619 acc : 0.791 \n",
      "Test result of epoch 52/100 || loss : 0.583 acc : 0.801 \n",
      "Test result of epoch 53/100 || loss : 0.683 acc : 0.770 \n",
      "Test result of epoch 54/100 || loss : 0.598 acc : 0.792 \n",
      "Test result of epoch 55/100 || loss : 0.613 acc : 0.793 \n",
      "Test result of epoch 56/100 || loss : 0.617 acc : 0.785 \n",
      "Test result of epoch 57/100 || loss : 0.586 acc : 0.798 \n",
      "Test result of epoch 58/100 || loss : 0.625 acc : 0.791 \n",
      "Test result of epoch 59/100 || loss : 0.595 acc : 0.798 \n",
      "Test result of epoch 60/100 || loss : 0.561 acc : 0.804 \n",
      "Test result of epoch 61/100 || loss : 0.602 acc : 0.797 \n",
      "Test result of epoch 62/100 || loss : 0.636 acc : 0.790 \n",
      "Test result of epoch 63/100 || loss : 0.561 acc : 0.808 \n",
      "Test result of epoch 64/100 || loss : 0.627 acc : 0.787 \n",
      "Test result of epoch 65/100 || loss : 0.590 acc : 0.803 \n",
      "Test result of epoch 66/100 || loss : 0.597 acc : 0.802 \n",
      "Test result of epoch 67/100 || loss : 0.634 acc : 0.787 \n",
      "Test result of epoch 68/100 || loss : 0.559 acc : 0.814 \n",
      "Test result of epoch 69/100 || loss : 0.589 acc : 0.799 \n",
      "Test result of epoch 70/100 || loss : 0.562 acc : 0.808 \n",
      "Test result of epoch 71/100 || loss : 0.661 acc : 0.780 \n",
      "Test result of epoch 72/100 || loss : 0.612 acc : 0.791 \n",
      "Test result of epoch 73/100 || loss : 0.666 acc : 0.777 \n",
      "Test result of epoch 74/100 || loss : 0.580 acc : 0.804 \n",
      "Test result of epoch 75/100 || loss : 0.622 acc : 0.793 \n",
      "Test result of epoch 76/100 || loss : 0.613 acc : 0.796 \n",
      "Test result of epoch 77/100 || loss : 0.591 acc : 0.797 \n",
      "Test result of epoch 78/100 || loss : 0.622 acc : 0.797 \n",
      "Test result of epoch 79/100 || loss : 0.580 acc : 0.806 \n",
      "Test result of epoch 80/100 || loss : 0.525 acc : 0.819 \n",
      "Test result of epoch 81/100 || loss : 0.481 acc : 0.836 \n",
      "Test result of epoch 82/100 || loss : 0.516 acc : 0.827 \n",
      "Test result of epoch 83/100 || loss : 0.478 acc : 0.836 \n",
      "Test result of epoch 84/100 || loss : 0.501 acc : 0.831 \n",
      "Test result of epoch 85/100 || loss : 0.498 acc : 0.835 \n",
      "Test result of epoch 86/100 || loss : 0.497 acc : 0.831 \n",
      "Test result of epoch 87/100 || loss : 0.494 acc : 0.832 \n",
      "Test result of epoch 88/100 || loss : 0.486 acc : 0.836 \n",
      "Test result of epoch 89/100 || loss : 0.507 acc : 0.834 \n",
      "Test result of epoch 90/100 || loss : 0.503 acc : 0.833 \n",
      "Test result of epoch 91/100 || loss : 0.536 acc : 0.821 \n",
      "Test result of epoch 92/100 || loss : 0.504 acc : 0.831 \n",
      "Test result of epoch 93/100 || loss : 0.514 acc : 0.829 \n",
      "Test result of epoch 94/100 || loss : 0.524 acc : 0.827 \n",
      "Test result of epoch 95/100 || loss : 0.529 acc : 0.822 \n",
      "Test result of epoch 96/100 || loss : 0.525 acc : 0.828 \n",
      "Test result of epoch 97/100 || loss : 0.489 acc : 0.838 \n",
      "Test result of epoch 98/100 || loss : 0.499 acc : 0.834 \n",
      "Test result of epoch 99/100 || loss : 0.530 acc : 0.826 \n",
      "Best test accuracy of inception network : 0.838 took 3211.150 secs\n",
      "Best accuracy of mlp = 62.39%\n",
      "Best accuracy of conv = 82.16%\n",
      "Best accuracy of resPlain = 88.81%\n",
      "Best accuracy of resBottleneck = 86.66%\n",
      "Best accuracy of inception = 83.82%\n"
     ]
    }
   ],
   "source": [
    "final_accs = {}\n",
    "\n",
    "# Start training\n",
    "for block_type, net in zip(block_types, networks):\n",
    "    try:\n",
    "        args.name = block_type\n",
    "\n",
    "        # Define optimizer\n",
    "        optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.0001)\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,80], gamma=0.5)\n",
    "        \n",
    "        # Create directories for logs and ckechpoints.\n",
    "        ckpt_dir = parent_dir / args.name / args.ckpt_dir\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        log_dir = parent_dir / args.name / args.log_dir\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create tensorboard writer,\n",
    "        if args.tensorboard: \n",
    "            writer = SummaryWriter(log_dir)\n",
    "\n",
    "        # Call the train & test function.\n",
    "        t1 = time.time()\n",
    "        accuracy = train_net(net, optimizer, scheduler, block_type, writer)\n",
    "        t = time.time()-t1\n",
    "        print(f'Best test accuracy of {block_type} network : {accuracy:.3f} took {t:.3f} secs')\n",
    "        final_accs[f'{block_type}'] = accuracy*100\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# Print final best accuracies of the models.\n",
    "for key in final_accs.keys():\n",
    "    print(f'Best accuracy of {key} = {final_accs[key]:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3.Aggregating experimental results and number of model parameters. (10pt)\n",
    "\n",
    "In this section, we automatically collect the classification performance of trained model. Also, we will count the number of parameters in the models. \n",
    "You should match your own results with the values we provided. While the number of the parameters should be exactly same, classification accuarcy should be in the range of $\\pm$1.5% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_types = ['mlp', 'conv','resPlain','resBottleneck','inception']\n",
    "test_accs = {}\n",
    "test_params= {}\n",
    "\n",
    "for block_type, net in zip(block_types, networks):\n",
    "        ckpt_dir = parent_dir / block_type / args.ckpt_dir\n",
    "        \n",
    "        # load weights from best checkpoints.\n",
    "        ckpt_path = f'{ckpt_dir}/{block_type}_best.pt'\n",
    "        try:\n",
    "            net.load_state_dict(torch.load(ckpt_path))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # Measure test performance.\n",
    "        net.eval() \n",
    "        with torch.no_grad():\n",
    "            test_accuracy = 0.\n",
    "            test_num_data = 0.\n",
    "            for batch_idx, (x, y) in enumerate(test_dataloader):\n",
    "                # Send `x` and `y` to either cpu or gpu using `device` variable..\n",
    "                x = x.to(device=device)\n",
    "                y = y.to(device=device)\n",
    "\n",
    "                # Feed `x` into the network, get an output, and keep it in a variable called `logit`.\n",
    "                logit = net(x)\n",
    "\n",
    "                # Compute loss using `logit` and `y`, and keep it in a variable called `loss`.\n",
    "                loss = nn.CrossEntropyLoss()(logit, y)\n",
    "\n",
    "                # Compute accuracy of this batch using `logit`, and keep it in a variable called 'accuracy'.\n",
    "                accuracy = (logit.argmax(dim=1) == y).float().mean()\n",
    "\n",
    "                test_accuracy += accuracy.item()*x.shape[0]\n",
    "                test_num_data += x.shape[0]\n",
    "\n",
    "            # Average classification accuracy.\n",
    "            test_accuracy /= test_num_data\n",
    "\n",
    "            # Count the number of implemented models.\n",
    "            num_parameters = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "\n",
    "            test_accs[f'{block_type}'] = test_accuracy*100\n",
    "            test_params[f'{block_type}'] = num_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Method        | Accuracy   | # Params    | Expected Acc | Expected # Params  \n",
      "------------------------------------------------------------------------------\n",
      " mlp           | 62.39      | 1649354     | 62.6         | 1649354     \n",
      " conv          | 82.16      | 510426      | 81.9         | 510426      \n",
      " resPlain      | 88.81      | 510426      | 88.6         | 510426      \n",
      " resBottleneck | 86.66      | 113946      | 86.5         | 113946      \n",
      " inception     | 83.82      | 124026      | 83.7         | 124026      \n"
     ]
    }
   ],
   "source": [
    "# Printing final results.\n",
    "correct_accs = {'mlp' : 62.6,'conv' : 81.9,'resPlain' : 88.6, 'resBottleneck' : 86.5, 'inception' : 83.7}\n",
    "correct_params = {'mlp' : 1649354, 'conv' : 510426, 'resPlain' : 510426, 'resBottleneck' : 113946, 'inception' : 124026}\n",
    "\n",
    "print(' Method        | Accuracy   | # Params    | Expected Acc | Expected # Params  ')\n",
    "print('------------------------------------------------------------------------------')\n",
    "for block in block_types:\n",
    "        print(f' {block:14}| {str(test_accs[block])[:5]:11}| {str(test_params[block]):11} | {str(correct_accs[block])[:5]:13}| {str(correct_params[block]):12}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "I3NY7sfLled4"
   ],
   "machine_shape": "hm",
   "name": "20200434.ipynb",
   "provenance": [
    {
     "file_id": "1LG-5iv9kUu-ZMXDNMEKDEbDEONnSAbT6",
     "timestamp": 1633008346906
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fddf6d32d1a49ebba007e0f1ee87510": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e92369e906a45689c9667f16e1f59bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbf7991179a546d3a094ffeff9fca8e4",
      "placeholder": "​",
      "style": "IPY_MODEL_c9492caa16964a97b9e61f5797515173",
      "value": " 170499072/? [00:03&lt;00:00, 48290985.98it/s]"
     }
    },
    "5e0bd028762243289de1939e81be6dbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81c045edb02f48a7bc76aa717f4d6d16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fddf6d32d1a49ebba007e0f1ee87510",
      "placeholder": "​",
      "style": "IPY_MODEL_d931503d74f14e90a4e8432e8fab9c34",
      "value": ""
     }
    },
    "9a61097dc9634ffcaa7f191cd96c1b09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a3b3f5f8274d459ca46de0ea61de4cb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81c045edb02f48a7bc76aa717f4d6d16",
       "IPY_MODEL_cc0aef71a70b44e7b9ed06b64a556f74",
       "IPY_MODEL_4e92369e906a45689c9667f16e1f59bd"
      ],
      "layout": "IPY_MODEL_5e0bd028762243289de1939e81be6dbf"
     }
    },
    "c9492caa16964a97b9e61f5797515173": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc0aef71a70b44e7b9ed06b64a556f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1eeaea7a62242dc9efc79cd7842df37",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a61097dc9634ffcaa7f191cd96c1b09",
      "value": 170498071
     }
    },
    "d1eeaea7a62242dc9efc79cd7842df37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d931503d74f14e90a4e8432e8fab9c34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbf7991179a546d3a094ffeff9fca8e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
